<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kangfu MEI's Homepage</title>

  <meta name="author" content="Kangfu MEI">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kangfu MEI</name>
                  </p>
                  <p>I am a Ph.D. student at <a href="hhttps://www.jhu.edu"> Johns Hopkins University, advised by <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/"> Prof. Vishal Patel</a>, where I work on compute vision and computational photography.
                    Before that, I was a M.Phil student at <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen</a>, advised by <a href="https://myweb.cuhk.edu.cn/ruihuang"> Prof. Rui Huang</a>.
                  </p>
                  <p>
                    I previously interned at: <a href="https://damo.alibaba.com/?lang=en">DAMO Academy</a> &nbsp/&nbsp
                    <a href="http://www.kwai.com/aboutus">Kuaishou</a> &nbsp/&nbsp <a
                      href="https://www.jddglobal.com/en/">JD.COM</a>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:kangfumei@link.cuhk.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="data/Kangfu_CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=e_nu_TIAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/MKFMIKU">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/mkfmiku">Zhihu</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/Kangfu.jpg"><img style="width:80%;max-width:80%;" alt="profile photo"
                      src="images/Kangfu.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research interest mainly focuses on degraded images restoration as well as its applicatoin in
                    high-level vision.
                    Representative papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="sdan_stop()" onmouseover="sdan_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='sdan_image'>
                      <img src='images/sdan_after.jpg' width="160">
                    </div>
                    <img src='images/sdan_before.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function sdan_start() {
                      document.getElementById('sdan_image').style.opacity = "1";
                    }

                    function sdan_stop() {
                      document.getElementById('sdan_image').style.opacity = "0";
                    }
                    sdan_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2104.00848">
                    <papertitle>SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Shenglong Ye,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>ICME</em>, 2021
                  <br>
                  <a href="https://github.com/MKFMIKU/SDAN">code</a> /
                  <a href="https://arxiv.org/abs/2104.00848">arXiv</a>
                  <p></p>
                  <p>New state of the art of LearnZoom dataset. A squared deformable network is proposed for the
                    misaligned image restoration learning.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/attanet.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function dpl_start() {
                      document.getElementById('dpl_image').style.opacity = "1";
                    }

                    function dpl_stop() {
                      document.getElementById('dpl_image').style.opacity = "0";
                    }
                    dpl_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songqi-github.github.io/research/AttaNet/index.html">
                    <papertitle>AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing</papertitle>
                  </a>
                  <br>
                  Qi Song, <strong>Kangfu Mei</strong>,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>AAAI</em>, 2021
                  <br>
                  <a href="https://github.com/songqi-github/AttaNet">code</a> /
                  <a href="https://arxiv.org/abs/2103.05930">arXiv</a>
                  <p></p>
                  <p>Two novel Strip Attention Module (SAM) and Attention Fusion Module (AFM) are proposed for enhancing
                    the accuracy of semantic segmentation networks with limited computational complexity increasing.
                    , espcically the scenes contains vertical strip areas</p>
                </td>
              </tr>


              <!-- <tr onmouseout="dpl_stop()" onmouseover="dpl_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dpl_image'>
                      <img src='images/dpl_our.png' width="160">
                    </div>
                    <img src='images/dpl_cx.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function dpl_start() {
                      document.getElementById('dpl_image').style.opacity = "1";
                    }

                    function dpl_stop() {
                      document.getElementById('dpl_image').style.opacity = "0";
                    }
                    dpl_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="research/dpl/index.html">
                    <papertitle>Disentangle Perceptual Learning through Online Contrastive Learning</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Yao Lu, Qiaosi Yi, Haoyu Wu,
                  <a href="https://junchenglee.com/">Juncheng Li</a>,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>arXiv</em>, 2020
                  <br>
                  <a href="#">Work in Progress</a> /
                  <a href="research/dpl/index.html">project page</a> /
                  <a href="https://arxiv.org/abs/2006.13511">arXiv</a>
                  <p></p>
                  <p>Enhance the realism of resotred images using a simple contrastive learning module that disentangles
                    perception-relevant dimensions in self-supervised manner.</p>
                </td>
              </tr> -->

              <tr onmouseout="spin_stop()" onmouseover="spin_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='spin_image'>
                      <img src='images/spin_after.png' width="160">
                    </div>
                    <img src='images/spin_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function spin_start() {
                      document.getElementById('spin_image').style.opacity = "1";
                    }

                    function spin_stop() {
                      document.getElementById('spin_image').style.opacity = "0";
                    }
                    spin_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="#">
                    <papertitle>Spatial Pixel Irrelevant Network for Learning Image-to-image Translation</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Haoyu Wu,
                  <a href="https://junchenglee.com/">Juncheng Li</a>,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>arXiv</em>, 2020
                  <br>
                  <a href="#">Work in Progress</a>
                  <br>
                  <p></p>
                  <p>
                    Enable networks to learn the image translation without aligned pairs and achieve the
                    state-of-the-art results in the real-world super-resolution task.
                  </p>
                </td>
              </tr>

              <tr onmouseout="dounet_stop()" onmouseover="dounet_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dounet_image'>
                      <img src='images/dounet_after.png' width="160">
                    </div>
                    <img src='images/dounet_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function dounet_start() {
                      document.getElementById('dounet_image').style.opacity = "1";
                    }

                    function dounet_stop() {
                      document.getElementById('dounet_image').style.opacity = "0";
                    }
                    dounet_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="#">
                    <papertitle>Visual and Semantic Scene Understanding Under Bad Weather</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Haoyu Wu,
                  <a href="https://junchenglee.com/">Juncheng Li</a>,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>arXiv</em>, 2019
                  <br>
                  <a href="#">Work in Progress</a>
                  <br>
                  <p></p>
                  <p>
                    Address the semantic segmentation accuracy dropping problem in dehazed images using the proposed
                    semantic guiding loss.
                  </p>
                </td>
              </tr>

              <tr onmouseout="hern_stop()" onmouseover="hern_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='hern_image'>
                      <img src='images/hern_after.jpg' width="160">
                    </div>
                    <img src='images/hern_before.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function hern_start() {
                      document.getElementById('hern_image').style.opacity = "1";
                    }

                    function hern_stop() {
                      document.getElementById('hern_image').style.opacity = "0";
                    }
                    hern_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1911.08098">
                    <papertitle>Higher-resolution Network for Image Demosaicing and Enhancing</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>,
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Jiajie Zhang, Haoyu Wu, Jie Li,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>ICCV Worshop</em>, 2019 &nbsp <font color="red"><strong>(AIM2019 Winner Award)</strong></font>
                  <br>
                  <a href="https://github.com/MKFMIKU/HERN-Pytorch">code</a> /
                  <a href="data/mei2019higher.bib">bibtex</a>
                  <br>
                  <p></p>
                  <p>
                    Achieved the best visual quality in Advances in Image Manipulation Workshop (AIM2019) Challenge on
                    RAW to RGB Mapping.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/srrfn.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content_ICCVW_2019/html/LCI/Li_Lightweight_and_Accurate_Recursive_Fractal_Network_for_Image_Super-Resolution_ICCVW_2019_paper.html">
                    <papertitle>Lightweight and Accurate Recursive Fractal Network for Image Super-Resolution
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Yiting Yuan,
                  <strong>Kangfu Mei</strong>, Faming Fang
                  <br>
                  <em>ICCV Worshop</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://github.com/MIVRC/SRRFN-PyTorch">code</a> /
                  <a href="data/li2019lightweight.bib">bibtex</a>
                  <br>
                  <p></p>
                  <p>
                    A lightweight Super-resolution framework that can construct infinitely possible topological
                    sub-structure through Fractal Tree.
                  </p>
                </td>
              </tr>

              <tr onmouseout="pffnet_stop()" onmouseover="pffnet_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='pffnet_image'>
                      <img src='images/pffnet_after.png' width="160">
                    </div>
                    <img src='images/pffnet_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function pffnet_start() {
                      document.getElementById('pffnet_image').style.opacity = "1";
                    }

                    function pffnet_stop() {
                      document.getElementById('pffnet_image').style.opacity = "0";
                    }
                    pffnet_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1810.02283">
                    <papertitle>Progressive Feature Fusion Network for Realistic Image Dehazing</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Aiwen Jiang,
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Mingwen Wang
                  <br>
                  <em>ACCV</em>, 2018
                  <br>
                  <a href="https://github.com/MKFMIKU/PFFNet">code</a> /
                  <a href="data/mei2018progressive.bib">bibtex</a>
                  <p></p>
                  <p>The first proposed haze removal method that removes haze from a single image in the complete
                    end-to-end manner.</p>
                </td>
              </tr>

              <tr onmouseout="rpm_stop()" onmouseover="rpm_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='rpm_image'>
                      <img src='images/rpm_after.png' width="160">
                    </div>
                    <img src='images/rpm_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function rpm_start() {
                      document.getElementById('rpm_image').style.opacity = "1";
                    }

                    function rpm_stop() {
                      document.getElementById('rpm_image').style.opacity = "0";
                    }
                    rpm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/8695183">
                    <papertitle>Deep Residual Refining based Pseudo-multi-frame Network for Effective Single Image
                      Super-resolution</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Aiwen Jiang,
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Bo Liu, Jihua Ye, Mingwen Wang
                  <br>
                  <em>IET Image Processing</em>, 2018
                  <br>
                  <a href="https://github.com/MKFMIKU/RPMNet">code</a> /
                  <a href="data/mei2018deep.bib">bibtex</a>
                  <p></p>
                  <p>The first proposed haze removal method that removes haze from a single image in the complete
                    end-to-end manner.</p>
                </td>
              </tr>


              <tr onmouseout="msrn_stop()" onmouseover="msrn_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='msrn_image'>
                      <img src='images/msrn_after.png' width="160">
                    </div>
                    <img src='images/msrn_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function msrn_start() {
                      document.getElementById('msrn_image').style.opacity = "1";
                    }

                    function msrn_stop() {
                      document.getElementById('msrn_image').style.opacity = "0";
                    }
                    msrn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content_ECCV_2018/html/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.html">
                    <papertitle>Multi-scale Residual Network for Image Super-resolution</papertitle>
                  </a>
                  <br>
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Faming Fang,
                  <strong>Kangfu Mei</strong>, Guixu Zhang
                  <br>
                  <em>ECCV</em>, 2018
                  <br>
                  <a href="https://github.com/MIVRC/MSRN-PyTorch">code</a> /
                  <a href="data/li2018multi.bib">bibtex</a>
                  <p></p>
                  <p>Introduce a novel multi-scale residual network for recovering the high-quality image from
                    low-resolution.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/srsenet.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1810.01831">
                    <papertitle>Effective Single-image Super-resolution Model using Squeeze-and-excitation Networks
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>, Aiwen Jiang,
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Jihua Ye, Mingwen Wang
                  <br>
                  <em>ICNOIP</em>, 2018
                  <br>
                  <a href="https://arxiv.org/abs/1810.01831">arXiv</a> /
                  <a href="https://github.com/MKFMIKU/SrSENet">code</a> /
                  <a href="data/mei2018effective.bib">bibtex</a>
                  <p></p>
                  <p>Using squeeze-and-excitation blocks to improve the visual quality of super-resolution results with
                    finer texture details.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Service</heading>
                  <p>Reviewer of International Journal of Computer Vision (IJCV)</p>
                  <p>Reviewer of IEEE Transactions on Image Processing (TIP)</p>
                  <p>Reviewer of IEEE Transactions on Multimedia (TMM)</p>
                  <p>Reviewer of IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</p>
                  <p>Reviewer of Computer Vision and Image Understanding (CVIU)</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Prizes</heading>
                  <p style="color: crimson">AIM2019 Mobile Raw to DSLR RGB Image Mapping Challenge (ICCV2019 Workshop):
                    Top 1</p>
                  <p>Alibaba Youku Video Enhancement and Super-Resolution Challenge 2019: Top 4</p>
                  <p>NTIRE2018 Image Dehazing Challenge (CVPR2018 Workshop): Honorable Mention Award & Top 6</p>
                  <p style="color: crimson">University Computer Software Programming Challenge 2018 in The Pearl River
                    Delta: Gold Award & Best innovative Award</p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
