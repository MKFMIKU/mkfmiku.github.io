<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kangfu MEI's Homepage</title>

  <meta name="author" content="Kangfu MEI">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="preconnect" href="https://fonts.googleapis.com"> 
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
  <link href="https://fonts.googleapis.com/css2?family=Ma+Shan+Zheng&display=swap" rel="stylesheet">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kangfu MEI <span style="font-family: 'Ma Shan Zheng', cursive;">梅 康夫</span></name>
                  </p>
                  <p>I am a Ph.D. student at <a href="hhttps://www.jhu.edu"> Johns Hopkins University </a>, advised by <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/"> Prof. Vishal Patel </a>, where I work on compute vision and computational photography.
                    Before that, I was a M.Phil student at <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen</a>, advised by <a href="https://myweb.cuhk.edu.cn/ruihuang"> Prof. Rui Huang</a>.
                  </p>
                  <p>
                    I'm working as a research intern at Adobe Research <img src="https://img.icons8.com/external-those-icons-lineal-color-those-icons/24/null/external-Adobe-logos-and-brands-those-icons-lineal-color-those-icons.png"/> (<i>Summer 2022</i>).
                  </p>
                  <p>
                    I previously interned at: <a href="https://damo.alibaba.com/?lang=en">DAMO Academy</a> <img src="https://img.icons8.com/color/24/null/taobao.png"/> &nbsp/&nbsp
                    <a href="http://www.kwai.com/aboutus">Kuaishou</a> <img src="https://img.icons8.com/external-regular-kawalan-studio/24/ff5000/external-kwai-social-media-regular-kawalan-studio.png"/> &nbsp/&nbsp
                    <a href="https://www.jddglobal.com/en/">JD.COM</a>
                  </p>
                  <p style="text-align:center">
                    <a href="Kangfu_CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="mailto:mikumkf@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=e_nu_TIAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/MKFMIKU">Github</a> &nbsp/&nbsp
                    <a href="https://photography.kfmei.page"><span class="highlight">Photography</span></a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/Kangfu.jpg"><img style="width:80%;max-width:80%;" alt="profile photo"
                      src="images/Kangfu.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research interest mainly focuses on degraded images restoration as well as its applicatoin in
                    high-level vision.
                    Representative papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="vidm_stop()" onmouseover="vidm_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='vidm_image'>
                      <img src='vidm/results/taichi/vidm.gif' width="160">
                    </div>
                    <img src='vidm/results/sky/vidm.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function vidm_start() {
                      document.getElementById('vidm_image').style.opacity = "1";
                    }

                    function vidm_stop() {
                      document.getElementById('vidm_image').style.opacity = "0";
                    }
                    d2sm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="vidm/">
                    <papertitle>VIDM: Video Implicit Diffusion Models</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>,
                  <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/">Vishal M. Patel</a>,
                  <br>
                  <em>AAAI</em>, 2022
                  <br>
                  <a href="vidm/">Project Page</a> /
                  <a href="#">arXiv (Coming Soon)</a> 
                  <p></p>
                  <p>Video Generation Diffusion Models By Using Implicit Motiion Condition.</p>
                </td>
              </tr>

              <tr onmouseout="d2sm_stop()" onmouseover="d2sm_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='d2sm_image'>
                      <img src='images/d2sm_after.png' width="160">
                    </div>
                    <img src='images/d2sm_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function d2sm_start() {
                      document.getElementById('d2sm_image').style.opacity = "1";
                    }

                    function d2sm_stop() {
                      document.getElementById('d2sm_image').style.opacity = "0";
                    }
                    d2sm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="d2sm">
                    <papertitle>Deep Semantic Statistics Matching (D2SM) Denoising Network</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>,
                  <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/">Vishal M. Patel</a>,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="d2sm">Project Page</a> /
                  <a href="https://arxiv.org/abs/2207.09302">arXiv</a> / 
                  <a href="d2sm/d2sm-poster.pdf">poster</a>
                  <p></p>
                  <p>A New General Plug-and-play Component For Denoising</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/lttgan.jpg' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://kfmei.page/LTT-GAN/">
                    <papertitle>LTT-GAN: Looking Through Turbulence by Inverting GANs</papertitle>
                  </a>
                  <br>
                  <strong>Kangfu Mei</strong>,
                  <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/">Vishal M. Patel</a>
                  <br>
                  <em>arXiv</em>, 2021
                  <br>
                  <a href="https://kfmei.page/LTT-GAN/">Project Page</a> /
                  <a href="https://arxiv.org/abs/2112.02379">arXiv</a>
                  <p></p>
                  <p>The first turbulence mitigation algorithm that can clearly recover face images captured in a range of 300 meters long.</p>
                </td>
              </tr>
              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/attanet.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function dpl_start() {
                      document.getElementById('dpl_image').style.opacity = "1";
                    }

                    function dpl_stop() {
                      document.getElementById('dpl_image').style.opacity = "0";
                    }
                    dpl_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songqi-github.github.io/research/AttaNet/index.html">
                    <papertitle>AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing</papertitle>
                  </a>
                  <br>
                  Qi Song, <strong>Kangfu Mei</strong>,
                  <a href="https://myweb.cuhk.edu.cn/ruihuang">Rui Huang</a>
                  <br>
                  <em>AAAI</em>, 2021
                  <br>
                  <a href="https://github.com/songqi-github/AttaNet">code</a> /
                  <a href="https://arxiv.org/abs/2103.05930">arXiv</a>
                  <p></p>
                  <p>Two novel Strip Attention Module (SAM) and Attention Fusion Module (AFM) are proposed for enhancing
                    the accuracy of semantic segmentation networks with limited computational complexity increasing.
                    , espcically the scenes contains vertical strip areas</p>
                </td>
              </tr>

              <tr onmouseout="msrn_stop()" onmouseover="msrn_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='msrn_image'>
                      <img src='images/msrn_after.png' width="160">
                    </div>
                    <img src='images/msrn_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function msrn_start() {
                      document.getElementById('msrn_image').style.opacity = "1";
                    }

                    function msrn_stop() {
                      document.getElementById('msrn_image').style.opacity = "0";
                    }
                    msrn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content_ECCV_2018/html/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.html">
                    <papertitle>Multi-scale Residual Network for Image Super-resolution</papertitle>
                  </a>
                  <br>
                  <a href="https://junchenglee.com/">Juncheng Li</a>, Faming Fang,
                  <strong>Kangfu Mei</strong>, Guixu Zhang
                  <br>
                  <em>ECCV</em>, 2018
                  <br>
                  <a href="https://github.com/MIVRC/MSRN-PyTorch">code</a> /
                  <a href="data/li2018multi.bib">bibtex</a>
                  <p></p>
                  <p>Introduce a novel multi-scale residual network for recovering the high-quality image from
                    low-resolution.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Service</heading>
                  <p>Reviewer for CVPR, ECCV, WACV, ICPR</p>
                  <p>Reviewer of International Journal of Computer Vision (IJCV)</p>
                  <p>Reviewer of IEEE Transactions on Image Processing (TIP)</p>
                  <p>Reviewer of IEEE Transactions on Multimedia (TMM)</p>
                  <p>Reviewer of IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</p>
                  <p>Reviewer of Computer Vision and Image Understanding (CVIU)</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Prizes</heading>
                  <p style="color: crimson">AIM2019 Mobile Raw to DSLR RGB Image Mapping Challenge (ICCV2019 Workshop):
                    Top 1</p>
                  <p>Alibaba Youku Video Enhancement and Super-Resolution Challenge 2019: Top 4</p>
                  <p>NTIRE2018 Image Dehazing Challenge (CVPR2018 Workshop): Honorable Mention Award & Top 6</p>
                  <p style="color: crimson">University Computer Software Programming Challenge 2018 in The Pearl River
                    Delta: Gold Award & Best innovative Award</p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
